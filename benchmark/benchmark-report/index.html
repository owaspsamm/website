<!doctype html><html lang><head><meta charset=utf-8><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta name=viewport content="width=device-width,initial-scale=1"><title>The SAMM Benchmark Report</title><meta name=author content><meta name=keywords content="benchmark,what is,questions"><meta name=description content="OWASP SAMM Benchmark Report"><meta property="og:title" content="The SAMM Benchmark Report"><meta property="og:description" content="OWASP SAMM Benchmark Report"><meta property="og:type" content="article"><meta property="og:url" content="https://owaspsamm.org/benchmark/benchmark-report/"><meta property="og:image" content="https://owaspsamm.org/img/site-feature-image.png"><meta property="article:section" content="benchmark"><meta name=generator content="Hugo 0.81.0"><link href="//fonts.googleapis.com/css?family=Roboto:400,100,100italic,300,300italic,500,700,800" rel=stylesheet type=text/css><link rel=stylesheet href=//use.fontawesome.com/releases/v6.4.2/css/all.css><link rel=stylesheet href=//use.fontawesome.com/releases/v6.4.2/css/v4-shims.css><link rel=stylesheet href=//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css integrity=sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u crossorigin=anonymous><link href=/css/animate.css rel=stylesheet><link href=/css/style.samm.css rel=stylesheet id=theme-stylesheet><link href=/css/custom.css rel=stylesheet><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script><script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel="shortcut icon" href=/img/favicon.ico type=image/x-icon><link rel=apple-touch-icon href=/img/apple-touch-icon.png><link href=/css/owl.carousel.css rel=stylesheet><link href=/css/owl.theme.css rel=stylesheet><link rel=alternate href=https://owaspsamm.org/index.xml type=application/rss+xml title="OWASP SAMM"><meta property="og:title" content="The SAMM Benchmark Report"><meta property="og:type" content="website"><meta property="og:url" content="/benchmark/benchmark-report//"><meta property="og:image" content="/img/logo.png"><link rel=stylesheet type=text/css href=https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.css></head><body><div id=all><header><div class=navbar-affixed-top data-spy=affix data-offset-top=200><div class="navbar navbar-default yamm" role=navigation id=navbar><div class=container><div class=navbar-header><a class="navbar-brand home" href=https://owaspsamm.org/><img src=https://owaspsamm.org//img/logo.png alt="The SAMM Benchmark Report logo" class="hidden-xs hidden-sm">
<img src=https://owaspsamm.org//img/logo-small.png alt="The SAMM Benchmark Report logo" class="visible-xs visible-sm">
<span class=sr-only>The SAMM Benchmark Report - go to homepage</span></a><div class=navbar-buttons><button type=button class="navbar-toggle btn-template-main" data-toggle=collapse data-target=#navigation>
<span class=sr-only>Toggle Navigation</span>
<i class="fa fa-align-justify"></i></button></div></div><div class="navbar-collapse collapse" id=navigation><ul class="nav navbar-nav navbar-right"><li class=dropdown><a href=# class=dropdown-toggle data-toggle=dropdown role=button aria-haspopup=true aria-expanded=false>About SAMM <span class=caret></span></a><ul class=dropdown-menu><li><a href=/about/>What is SAMM</a></li><li><a href=/team/>The team</a></li></ul></li><li class=dropdown><a href=/model/>The model</a></li><li class=dropdown><a href=/resources/>Resources</a></li><li class=dropdown><a href=# class=dropdown-toggle data-toggle=dropdown role=button aria-haspopup=true aria-expanded=false>Guidance <span class=caret></span></a><ul class=dropdown-menu><li><a href=/guidance/quick-start-guide/>Getting started</a></li><li><a href=/assessment/>Assessment</a></li><li><a href=/guidance/agile/>Agile</a></li><li><a href=/benchmark/>Benchmark</a></li><li><a href=/stream-guidance/>Stream guidance</a></li></ul></li><li class=dropdown><a href=# class=dropdown-toggle data-toggle=dropdown role=button aria-haspopup=true aria-expanded=false>Community <span class=caret></span></a><ul class=dropdown-menu><li><a href=/blog/>Blog</a></li><li><a href=/user-day/>User Day</a></li><li><a href=/sponsors/>Sponsors</a></li><li><a href=/samm-users/>Users</a></li><li><a href=/practitioners/>Practitioners</a></li><li><a href=/faq/>FAQ</a></li><li><a href=/contributing/>Contributing</a></li><li><a href=/contact/>Contact</a></li></ul></li></ul></div><div class="collapse clearfix" id=search><form class=navbar-form role=search><div class=input-group><input type=text class=form-control placeholder=Search>
<span class=input-group-btn><button type=submit class="btn btn-template-main"><i class="fa fa-search"></i></button></span></div></form></div></div></div></div></header><div id=heading-breadcrumbs><div class=container><div class=row><div class=col-md-12><h1>The SAMM Benchmark Report</h1></div></div></div></div><div id=content><div class=container><div class=row><div class="col-md-12 samm-page-content"><div class=samm-content-with-space><h2 id=goals>Goals</h2><p>The OWASP Software Assurance Maturity Model (SAMM) is rapidly becoming the go-to framework for application security programs, and it’s easy to see why. SAMM offers a structured, measurement-driven approach to improving software security. It helps organizations assess and elevate their software security maturity on a scale from 0 to 3. However, many organizations face challenges after completing their initial SAMM assessment. There’s growing interest in understanding how other organizations are progressing, making real-world data one of the hottest topics in the SAMM community. The SAMM Benchmark initiative is designed to address these challenges by offering a centralized source of real-world data, enabling organizations to compare their progress, identify trends, and learn from the broader community. In this first Benchmark Report, we provide an in-depth analysis and interpretation of the OWASP SAMM Benchmark data.</p></br><h2 id=key-takeaways>Key Takeaways</h2><ul><li><p><strong>Limited Dataset Scope</strong><br>Only 30 assessments, mostly from global multinationals, evaluated by third-party experts.</p></li><li><p><strong>Skewed Averages</strong><br>High SAMM average score of 1.44/3.0 likely unrepresentative of smaller firms.</p></li><li><p><strong>Governance and Operations</strong><br>Large organizations excel, driven by compliance requirements and mature processes.</p></li><li><p><strong>Verification Challenges</strong><br>Security requirements are defined but not systematically addressed in code.</p></li><li><p><strong>Tool Tuning</strong><br>Few organizations reduce SAST tool noise through proper tuning.</p></li><li><p><strong>Metrics Gap</strong><br>Investments in security lack clear ROI metrics, making effectiveness hard to measure.</p></li></ul></br></br><h2 id=benchmark-demographics>Benchmark Demographics</h2><h3 id=the-benchmark-dataset-is-limited-to-30-assessments>The Benchmark Dataset is Limited to 30 Assessments</h3><p>Let’s deal with the really bad news first. The current SAMM benchmark dataset includes only 30 submissions, significantly limiting its representativeness. With such a small sample size, drawing global insights is challenging. The Benchmark Initiative aims to provide segmented data based on parameters like geography, industry, company size, and development methodology. For instance, a bank would benefit most from comparing its scores with those of similar-sized banks in the same region. However, achieving this granularity requires adhering to k-anonymity, a privacy-preserving technique that ensures individual data cannot be distinguished from at least k other entries. On a positive note, the latest SAMM benchmark update supports segmentation by company size.</p><h3 id=most-data-comes-from-third-party-expert-assessments>Most Data Comes from Third-Party Expert Assessments</h3><p>Ensuring high-quality data is a central goal of the benchmark initiative. SAMM provides detailed criteria to help assessors evaluate each activity objectively. However, interpretations can vary, especially among self-assessments, which are often more optimistic. Over 80% of the dataset comes from independent expert third-party SAMM practitioners, ensuring data accuracy and consistency. This reliance on reputable assessors gives the benchmark data a strong foundation of credibility.<div class=responsive-image><img class=img-responsive src=https://owaspsamm.org//img/pages/benchmark/demographics-assessment-type.svg alt="SAMM Benchmark is mainly comprised of third-party assessments"></div></p><h3 id=a-majority-of-data-represents-global-multinational-companies>A Majority of Data Represents Global Multinational Companies</h3><p>Surprisingly, the majority of submissions come from large multinational companies operating globally, despite the challenges they face in securing legal approval to share their data. Smaller companies, which often have fewer legal and executive hurdles, are underrepresented. While the strong presence of multinationals is impressive, the dataset would benefit from more contributions by small and medium-sized organizations to better reflect the diversity of the application security landscape.4</p><h3 id=assessment-scope-reflects-a-balanced-mix>Assessment Scope Reflects a Balanced Mix</h3><p>The benchmark data represents a balanced mix of assessments conducted at the team, organizational, and company levels. Before conducting a SAMM assessment, defining the scope is crucial—whether it’s a single team, an entire company, or a specific business unit. While the scope itself currently does not appear to significantly impact results or their interpretation, this balance highlights the varied ways organizations are leveraging SAMM for their security maturity assessments.</br></br></p><h2 id=overall-benchmark-results>Overall benchmark results</h2><h3 id=maturity-levels>Maturity Levels</h3><p>Before diving into the scores, let’s revisit what the SAMM maturity levels represent.<div class=responsive-image><img class=img-responsive src=https://owaspsamm.org//img/pages/SAMM_model_structure.svg alt="SAMM Maturity Levels"></div></p><ul><li><strong>Maturity Level 0</strong><br>Indicates no activity or focus on application security for a given business function, practice, stream, or activity.</li><li><strong>Maturity Level 1</strong><br>Reflects an initial understanding and ad-hoc implementation of the practice.</li><li><strong>Maturity Level 2</strong><br>Represents increased efficiency and effectiveness in executing the practice.</li><li><strong>Maturity Level 3</strong><br>Signifies comprehensive mastery and implementation of the practice at scale.</li></ul></br><p>These maturity levels provide a qualitative framework for interpreting scores. However, averaging maturity levels across diverse practices can sometimes be misleading. For example, if three practices score 0.0, 3.0, and 3.0, their average would be 2.0, suggesting high maturity overall. Yet, a 0.0 score in a critical area—like secure builds—can have significant implications, diminishing the overall security posture.
Despite these nuances, averaging scores can still offer valuable insights. When practices are closely related, averaging becomes even more meaningful. For instance, averaging incident detection and incident response scores provides clearer insights than averaging governance and operations scores, which are less directly connected.</p><h3 id=average-samm-score-144-out-of-30>Average SAMM score: 1.44 out of 3.0</h3><p>The average SAMM score across assessments stands at 1.44 out of 3.0—a result that may seem unexpectedly high. Realistically, if we sampled 30 random companies, it’s unlikely their average would approach this level. Community feedback corroborates this skepticism, with many practitioners reporting significantly lower scores, particularly among smaller companies. Interestingly, the average score has risen compared to the previous benchmark dataset, which contained 25 assessments.</p><div class=responsive-image><img class=img-responsive src=https://owaspsamm.org//img/pages/benchmark/average-data.svg alt="SAMM Benchmark: average score"></div><h3 id=operations-the-highest-scoring-business-function>Operations: The Highest Scoring Business Function</h3><p>The Operations function achieved the highest score at 1.81 out of 3.0, nearing maturity level 2. Operations includes maintaining the confidentiality, integrity, and availability of applications and their data throughout their operational lifecycle. This strong performance is unsurprising, given the dataset’s heavy representation of large multinational organizations, which typically excel in operational maturity due to significant investments.
Surprisingly, small companies also scored relatively well in Operations. Despite having fewer resources, these companies probably leverage best practices and tools for operations tasks such as incident detection and response.</p><h3 id=design-the-second-highest-scorer>Design: The Second-Highest Scorer</h3><p>The Design function scored 1.5 out of 3.0, making it the second-highest business function. Design activities include defining goals and creating software securely, with a focus on processes such as threat modeling, security requirements, and reusable secure architecture. These activities are often associated with the &ldquo;shift left&rdquo; paradigm, which encourages addressing security considerations early in the development lifecycle.
While these practices demand significant effort and organizations don’t tend to invest in them, large companies often perform well due to their robust vendor management and technology management practices. Interestingly, smaller companies also scored respectably in Design, where it ranks as their third-highest scoring function. This suggests that even resource-constrained organizations can achieve meaningful progress in this area.</p><h3 id=implementation-strong-but-room-for-improvement>Implementation: Strong but Room for Improvement</h3><p>The Implementation function scored 1.46 out of 3.0. This area focuses on how organizations build and deploy software while managing defects. Given the rise of DevOps and DevSecOps, one might expect higher scores here. However, several challenges persist:</p><ul><li>Many organizations avoid failing builds even when vulnerabilities are detected.</li><li>Few teams adopt a “nice list” of approved third-party components.</li><li>Signed builds and signature verification during builds are rare.</li></ul><p>Small companies, despite resource limitations, see Implementation as their second-highest scoring function.</p><h3 id=governance-balancing-strengths-and-weaknesses>Governance: Balancing Strengths and Weaknesses</h3><p>Governance scored 1.35 out of 3.0 and encompasses activities for managing software development at an organizational level. Large organizations excel in governance, especially when it comes to policies, standards and regulatory compliance. Training and awareness initiatives are also gaining traction, lately. Smaller companies, however, often lack formal governance structures, relying instead on tribal knowledge. Programs like security champions or centralized knowledge bases remain uncommon in these environments, leading to a stark contrast in maturity scores between large and small organizations.</p><h3 id=verification-the-lowest-scoring-business-function>Verification: The Lowest Scoring Business Function</h3><p>Verification, with an average score of 1.12 out of 3.0, consistently ranks lowest. This function involves ensuring quality through activities like testing, reviews, and evaluations. Despite the critical role of verification in application security, organizations face several barriers:</p><ul><li>Tools are underutilized and rarely fine-tuned, leading to excessive false positives.</li><li>Threat modeling, though essential, is often misunderstood as overly complex and time-consuming.</li><li>Penetration testing outcomes are rarely integrated into broader security strategies, such as improved training, tuned tooling, or regression test suites.</li></ul><p>Verification’s low score highlights a broader issue: organizations are investing in security practices but often fail to systematically verify their effectiveness. Adopting pragmatic approaches to activities like threat modeling could help bridge this gap, fostering long-term improvements in application security programs.</p><h2 id=other-insights-from-the-samm-benchmark>Other Insights from the SAMM Benchmark</h2><h3 id=third-party-assessments-outperform-self-assessments>Third-Party Assessments Outperform Self-Assessments</h3><p>Surprisingly, third-party assessments score consistently higher than self-assessments. Initially, SAMM veterans expected self-assessments to inflate scores due to shallow interpretations of criteria and the tendency to overrate oneself. While these factors may still influence results, the data defies these simplistic expectations.</p><div class=responsive-image><img class=img-responsive src=https://owaspsamm.org//img/pages/benchmark/third-party-vs-self-assessments.svg alt="SAMM Benchmark: third party vs self-assessments"></div><p>One possible explanation is that teams often perceive SAMM assessments, regardless of the messaging, as audits. This perception motivates teams to prepare thoroughly, leading to improved scores. However, whether a team can significantly enhance their SAMM score in a short timeframe, such as two months, depends on various factors.</p><h3 id=fast-improvements-are-possible-for-certain-activities>Fast Improvements Are Possible for Certain Activities</h3><p>Some SAMM activities are relatively straightforward to implement and can boost maturity scores quickly. For instance, processes like secure build and deploy, centralized defect tracking, secrets management, and security testing often involve adopting and fine-tuning tools. Small organizations, in particular, might see rapid improvements by focusing on these areas before an anticipated third-party assessment.</p><h3 id=inflating-scores-without-real-progress>Inflating Scores Without Real Progress</h3><p>Unfortunately, it&rsquo;s also possible to artificially inflate SAMM scores without substantial work, particularly for practices where the current maturity level is low. For example, a team could quickly design a training program or update policies to appear more mature. However, this approach runs counter to the intent of SAMM, which emphasizes genuine security improvements.</p><h3 id=third-party-assessments-as-follow-ups>Third-Party Assessments as Follow-Ups</h3><p>Another factor is that third-party assessments often follow earlier self-assessments. Teams may conduct a self-assessment, address gaps, and then invite external experts to validate their progress. This sequence naturally leads to higher scores in third-party evaluations.</p><h3 id=challenges-with-metrics-and-kpis>Challenges with Metrics and KPIs</h3><p>SAMM encourages organizations to use metrics to measure the effectiveness of their application security programs. While many companies rely on metrics like vulnerability counts from SAST, DAST, or SCA tools, these are not always effective proxies for risk or program efficiency.</p><ul><li>Tools often produce false positives or fail to account for system context.</li><li>CVSS impact scores are poor substitutes for actual risk since they don&rsquo;t consider likelihood.</li><li>Metrics rarely reflect the broader return on investment in security or its impact on reducing real-world risks.</li></ul><h3 id=verification-of-security-requirements-is-rare>Verification of Security Requirements Is Rare</h3><p>Effective software engineering involves specifying, implementing, and verifying requirements are correctly implemented. Yet, many organizations fail to verify the correct implementation of security requirements systematically. While manual and automated tests can bridge this gap, they require varying levels of effort and investment.</p><div class=responsive-image><img class=img-responsive src=https://owaspsamm.org//img/pages/benchmark/security-testing-pyramid.svg alt="Security testing pyramid"></div><h3 id=threat-modeling-has-a-long-way-to-go>Threat Modeling Has a Long Way to Go</h3><p>Threat modeling is a crucial yet underused practice. Despite being relatively simple and cost-effective, it often lacks leadership buy-in, which hinders its adoption. With proper support, pragmatic threat modeling can significantly enhance security programs.</p><div class=responsive-image><img class=img-responsive src=https://owaspsamm.org//img/pages/benchmark/bottom-5-practices.svg alt="SAMM Benchmark: bottom 5 practices"></div><h3 id=security-tools-often-lack-proper-tuning>Security Tools Often Lack Proper Tuning</h3><p>Organizations frequently invest in security tools like SAST, DAST, and SCA but fail to optimize them. Untuned tools generate excessive findings, which can overwhelm teams and reduce effectiveness.</p><ul><li>Tools should be tested for compatibility with the tech stack before purchase.</li><li>Continuous tuning is essential to reduce false positives and integrate findings from manual reviews and penetration tests.</li></ul><h3 id=reluctance-to-block-builds-for-vulnerabilities>Reluctance to Block Builds for Vulnerabilities</h3><p>Many organizations hesitate to enforce build failures for vulnerabilities or vulnerable dependencies, as requested by SAMM Secure Build at maturity level 3. Although this is easy once you have a pipeline with integrated security tooling, an important reason could be that upgrading vulnerable third-party components can trigger a cascade of changes, making it a complex process. Also, while SAMM allows for risk acceptance strategies, these are rarely used.</p><h3 id=secrets-management-scores-are-strong>Secrets Management Scores Are Strong</h3><p>Secrets management stands out as a well-implemented area, thanks largely to modern best practices and tools. For example, tools now offering secret scanning have helped enforce disallowing secrets in the committed code. Cloud providers and security tools have made secrets management more accessible, contributing to higher scores.</p><h3 id=configuration-hardening-needs-better-monitoring>Configuration Hardening Needs Better Monitoring</h3><p>Configuration hardening ensures secure baseline configurations across components, but monitoring and enforcement often lag behind. Container image scanning frequently generates numerous findings, many of which are unresolvable due to outdated base images. Implementing “golden images” requires significant effort, and adoption can be challenging. Some teams rely on cloud providers for CIS benchmark monitoring, but this approach doesn’t fully address the complexities of diverse application stacks.</p><h3 id=data-catalog-adoption-remains-low>Data Catalog Adoption Remains Low</h3><p>Despite increasing regulatory requirements, data catalogs are still uncommon. Even teams strict about security practices sometimes overlook this foundational element, highlighting a gap in compliance-oriented data management.</br></br></p><h2 id=path-forward>Path Forward</h2><p>This benchmark underscores the importance of nuanced, sustained efforts to improve application security maturity. Organizations should focus on long-term strategies that integrate security into every stage of software development. The success in secrets management demonstrates the power of accessible tools and clear best practices, offering a model for tackling other challenging areas.</p><p>By addressing gaps in verification, embracing the full potential of threat modeling, and improving the alignment of metrics with meaningful outcomes, organizations can achieve more significant and lasting progress. As SAMM continues to evolve, it will remain a vital framework for guiding and measuring these efforts.</p></br><h2 id=submit-data>Submit data</h2><p>Remember this initiative is for organizations to measure themselves against their peers in the industry. Our goal remains to gather a large enough dataset to guarantee accurate comparisons and maintain full anonymity for the contributing parties.</p><p>There are 2 ways of submitting data</p><ul><li><p>Uploading it to the <a href=https://bit.ly/sammbenchmarksubmission target=_blank>Benchmark folder</a> <i class="link-icon fas fa-external-link-alt"></i><br>Please, refer to <a href="https://www.youtube.com/watch?v=zF4k0TXCvGw" target=_blank>this video</a> <i class="link-icon fas fa-external-link-alt"></i>for instructions.</p></li><li><p>Sending it by email to <a href=mailto:benchmark@owaspsamm.org>benchmark@owaspsamm.org</a></p></li></ul><p>The data is collected in an anonymous way and covered by the following <a href=benchmark-terms-and-conditions>terms and conditions</a>. During the submission process we will ask for some metadata. The more information provided, the better the comparative analysis will be.</p><p>To help practitioners get permission from their clients or companies to submit datasets, we have created the following <a href="https://docs.google.com/document/d/12Ryo0vwDsCpqJYtOA1FdhKnMl89yPpiJaAaAgopiUbg/edit?usp=sharing" target=_blank>email template</a> <i class="link-icon fas fa-external-link-alt"></i>.</p></br><div><a href=/benchmark class="btn btn-template-transparent-main btn-lg">Go to the main Benchmark page</a></div></div></div></div></div></div><footer id=footer><div class=container><div class="col-md-8 col-sm-6"><h4>About us</h4><p>This is an OWASP Project.</br>OWASP is an open community dedicated to enabling organizations to conceive, develop, acquire, operate, and maintain applications that can be trusted. All of the OWASP tools, documents, forums, and chapters are free and open to anyone interested in improving application security.</p><div class=social><a href=https://github.com/owaspsamm target=_blank style=opacity:1><i class="fab fa-2x fa-github"></i></a><a href=https://owasp.slack.com/messages/C0VF1EJGH target=_blank style=opacity:1><i class="fab fa-2x fa-slack"></i></a><a href=https://www.linkedin.com/company/owasp-samm/ target=_blank style=opacity:1><i class="fab fa-2x fa-linkedin-in"></i></a><a href=https://twitter.com/OwaspSAMM target=_blank style=opacity:1><i class="fa-brands fa-2x fa-x-twitter"></i></a><a href=https://www.meetup.com/owasp-samm/ target=_blank style=opacity:1><i class="fab fa-2x fa-meetup"></i></a><a href=https://www.youtube.com/channel/UCEZDbvQrj5APg5cEET49A_g target=_blank style=opacity:1><i class="fa fa-2x fa-youtube"></i></a><a href=mailto:info@owaspsamm.org target=_blank style=opacity:1><i class="fa fa-2x fa-envelope"></i></a></div><hr class="hidden-md hidden-lg hidden-sm"></div><div class="col-md-4 col-sm-6"><a href=https://owasp.org target=_blank><img src=https://owaspsamm.org//img/owasp_logo_1c_w_notext.png alt="The SAMM Benchmark Report"></a></div></div></footer><div id=copyright><div class=container><div class=col-md-12><p class=pull-left>OWASP SAMM is published under the
<a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA 4.0 license</a>
and we share the
<a href=https://owasp.org/www-policy/operational/privacy>OWASP Privacy Policy</a></p><p class=pull-right>Template by <a href=http://bootstrapious.com/free-templates>Bootstrapious</a>.
Ported to Hugo by <a href=https://github.com/devcows/hugo-universal-theme>DevCows</a></p></div></div></div></div><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-127457120-1','auto'),ga('send','pageview'))</script><script src=//code.jquery.com/jquery-3.1.1.min.js integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin=anonymous></script><script src=//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js integrity=sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa crossorigin=anonymous></script><script src=//cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/waypoints/4.0.1/jquery.waypoints.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/Counter-Up/1.0/jquery.counterup.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/jquery-parallax/1.1.3/jquery-parallax.js></script><script src="//maps.googleapis.com/maps/api/js?v=3.exp"></script><script src=https://owaspsamm.org/js/hpneo.gmaps.js></script><script src=https://owaspsamm.org/js/gmaps.init.js></script><script src=https://owaspsamm.org/js/front.js></script><script src=https://owaspsamm.org/js/tabs.js></script><script src=https://owaspsamm.org/js/owl.carousel.min.js></script><script src=https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.js data-cfasync=false></script><script>window.cookieconsent.initialise({palette:{popup:{background:"#66b8b8"},button:{background:"#366867"}},content:{message:"We use cookies mainly to analyze traffic. Some video providers also use cookies.",href:"https://owasp.org/www-policy/operational/privacy"}})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-44N5RHDT94"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-44N5RHDT94')</script></body></html>